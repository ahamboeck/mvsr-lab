{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 23:35:07.996322: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-10 23:35:07.997538: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 23:35:08.021281: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 23:35:08.021882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 23:35:08.447153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Reshape, InputLayer, Input, MaxPooling2D, Conv2D, Dense, Flatten, Dropout\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tensorflow version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file_path = '../data/train/outputs/image_descriptions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory for saved files\n",
    "output_dir = '../data/train/snippets/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "img_shape_full = (150, 150, 1)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all .npy files in the directory\n",
    "npy_files = [f for f in os.listdir(output_dir) if f.endswith('.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labels from the text file\n",
    "labels = {}\n",
    "with open(label_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        filename, label = line.strip().split(',')\n",
    "        labels[filename] = int(label)\n",
    "\n",
    "# Load images and their labels based on the filenames\n",
    "images = []\n",
    "image_labels = []\n",
    "for npy_file in sorted(os.listdir(output_dir)): # SUS\n",
    "    if npy_file.endswith('.npy') and npy_file in labels:\n",
    "        img_array = np.load(os.path.join(output_dir, npy_file))\n",
    "        images.append(img_array)\n",
    "        image_labels.append(labels[npy_file])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 29\n",
      "Number of labels: 29\n",
      "Image shape: (150, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images:\", len(images))\n",
    "print(\"Number of labels:\", len(image_labels))\n",
    "assert len(images) == len(image_labels)\n",
    "\n",
    "print(\"Image shape:\", images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_conv1 (Conv2D)        (None, 150, 150, 1)       10        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 150, 150, 1)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 75, 75, 1)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5625)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 11252     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11262 (43.99 KB)\n",
      "Trainable params: 11262 (43.99 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 23:35:09.239051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-10 23:35:09.239377: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Assuming all images are the same size and reshaped properly for input to a CNN\n",
    "images = np.array(images).reshape(-1, 150, 150, 1)  # Reshape for CNN, change shape as necessary\n",
    "\n",
    "# Start constructing the Keras Sequential model.\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer which is similar to a feed_dict in TensorFlow.\n",
    "model.add(InputLayer(input_shape=img_shape_full))\n",
    "\n",
    "# First convolutional layer with ReLU-activation and max-pooling.\n",
    "model.add(Conv2D(kernel_size=3, strides=1, filters=1, padding='same',\n",
    "                activation='relu', name='layer_conv1'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "# model.add(Conv2D(kernel_size=3, strides=1, filters=1, padding='same',\n",
    "                # activation='relu', name='layer_conv2'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Flatten the output of the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Last fully-connected layer with softmax-activation for use in classification.\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model (assuming this is for a classification task)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Show a summary of the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../models/model_dropout/'\n",
    "path = base_path + 'model_dropout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary saved to %s ../models/model_dropout/model_dropout.txt\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your Keras model\n",
    "\n",
    "# Open a file in write mode\n",
    "with open(path + '.txt', 'w') as f:\n",
    "    # Pass the file handle to the print function of model.summary()\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "print(\"Model summary saved to %s\", path + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays\n",
    "image_labels = np.array(image_labels)\n",
    "print(image_labels)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "image_labels = to_categorical(image_labels)\n",
    "print(image_labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, image_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize pixel values to mean 0 and standard deviation 1\n",
    "mean = np.mean(X_train, axis=(0, 1, 2), keepdims=True)\n",
    "std = np.std(X_train, axis=(0, 1, 2), keepdims=True)\n",
    "\n",
    "# Save to a file\n",
    "np.save(base_path + 'mean.npy', mean)\n",
    "np.save(base_path + 'std.npy', std)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.8400 - accuracy: 0.4000 - val_loss: 0.7452 - val_accuracy: 0.5556\n",
      "Epoch 2/6\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6696 - accuracy: 0.8000 - val_loss: 0.4369 - val_accuracy: 0.8889\n",
      "Epoch 3/6\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3510 - accuracy: 0.9500 - val_loss: 0.8570 - val_accuracy: 0.5556\n",
      "Epoch 4/6\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3319 - accuracy: 0.8000 - val_loss: 0.8336 - val_accuracy: 0.5556\n",
      "Epoch 5/6\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2707 - accuracy: 0.8000 - val_loss: 0.5180 - val_accuracy: 0.7778\n",
      "Epoch 6/6\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9000 - val_loss: 0.2812 - val_accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=5,\n",
    "    epochs=6,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)  # Use test data for validation\n",
    ")\n",
    "\n",
    "model.save(path + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Plots saved in the 'models' directory.\n"
     ]
    }
   ],
   "source": [
    "# Set larger font sizes and bold fonts across all plot elements\n",
    "plt.rc('font', size=28)  # controls default text sizes\n",
    "plt.rc('axes', titlesize=28, titleweight='bold', labelsize=24, labelweight='bold')  # fontsize of the axes title and labels\n",
    "plt.rc('xtick', labelsize=28)  # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=28)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=28)  # legend fontsize\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.plot(history.history['accuracy'], marker='o', linestyle='-', color='blue')\n",
    "plt.plot(history.history['val_accuracy'], marker='o', linestyle='--', color='green')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.savefig(path + '_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.plot(history.history['loss'], marker='o', linestyle='-', color='red')\n",
    "plt.plot(history.history['val_loss'], marker='o', linestyle='--', color='purple')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.savefig(path + '_loss.png')\n",
    "plt.close()\n",
    "\n",
    "# Predict the values from the test dataset\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1) \n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='viridis')  # Using 'viridis' for better visibility\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.savefig(path + '_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Plots saved in the 'models' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "model1 = load_model('../models/model_dropout_2_conv/model_dropout_2_conv.keras')\n",
    "model2 = load_model('../models/model_no_dropout_2_conv/model_no_dropout_2_conv.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4de839d820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_test and y_test are your test datasets\n",
    "y_pred1 = model1.predict(X_test)\n",
    "y_pred2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1_classes = np.argmax(y_pred1, axis=1)\n",
    "y_pred2_classes = np.argmax(y_pred2, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = confusion_matrix(y_true_classes, y_pred1_classes)\n",
    "cm2 = confusion_matrix(y_true_classes, y_pred2_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_path = '../models/confusion_matrices/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cm1).to_csv(confusion_matrix_path + 'model_dropout.csv', index=False)\n",
    "pd.DataFrame(cm2).to_csv(confusion_matrix_path + 'model_no_dropout.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the confusion matrices\n",
    "cm1 = pd.read_csv(confusion_matrix_path + 'model_dropout.csv').to_numpy()\n",
    "cm2 = pd.read_csv(confusion_matrix_path + 'model_no_dropout.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0]\n",
      " [0 2]]\n",
      "[[7 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(cm1)\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: 0\n",
      "c: 0\n",
      "McNemar's Test Statistic: inf\n",
      "P-value: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahamboeck/venv/keras/lib/python3.8/site-packages/statsmodels/stats/contingency_tables.py:1348: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n"
     ]
    }
   ],
   "source": [
    "# Extract b and c for McNemar's test\n",
    "# b: False Positives in cm1 not present in cm2\n",
    "# c: False Positives in cm2 not present in cm1\n",
    "b = cm1[0, 1] - cm2[0, 1]  # FP in Matrix 1 that are TP/TN in Matrix 2\n",
    "c = cm2[0, 1] - cm1[0, 1]  # FP in Matrix 2 that are TP/TN in Matrix 1\n",
    "\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "# Create the contingency table for McNemar's test\n",
    "contingency_table = np.array([[0, max(0, b)], [max(0, c), 0]])\n",
    "\n",
    "# Perform McNemar's test\n",
    "result = mcnemar(contingency_table, exact=False)\n",
    "print(\"McNemar's Test Statistic:\", result.statistic)\n",
    "print(\"P-value:\", result.pvalue)\n",
    "\n",
    "# Optional: Save the p-value to a file\n",
    "mcnemar_path = '../models/mcnemar_test_results/'\n",
    "with open(mcnemar_path + 'mcnemar_1_conv.txt', 'w') as file:\n",
    "    file.write(f\"McNemar's Test Statistic: {result.statistic}\\n\")\n",
    "    file.write(f\"P-value: {result.pvalue}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-cv-keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
